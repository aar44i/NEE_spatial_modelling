{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import seaborn as sns\n",
    "import shap\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objs as go\n",
    "import plotly.io as pio\n",
    "import plotly.express as px\n",
    "\n",
    "import catboost as cb\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "from sklearn.model_selection import train_test_split, KFold, TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, make_scorer, r2_score\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "import optuna\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreprocessing:\n",
    "    def __init__(self, forest_path, no_forest_path):\n",
    "        self.df_forest = pd.read_csv(forest_path)\n",
    "        self.df_no_forest = pd.read_csv(no_forest_path)\n",
    "        self.df = None\n",
    "\n",
    "    def load_and_clean_data(self):\n",
    "        # Drop 'Unnamed: 0' column\n",
    "        self.df_no_forest.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "        self.df_forest.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "\n",
    "        # Concatenate dataframes\n",
    "        self.df = pd.concat([self.df_forest, self.df_no_forest])\n",
    "\n",
    "        # Drop specific IGBP categories\n",
    "        IGBP_drop = ['BSV', 'WSA', 'SAV']\n",
    "        self.df = self.df[~self.df['IGBP'].isin(IGBP_drop)]\n",
    "\n",
    "        # Drop specific stations (Chosen by visual analysis)\n",
    "        valid_stations = [\n",
    "        'US_Ne1', 'BE_Vie', 'CA_Ca1', 'CA_Gro', 'CA_LP1', 'CA_NS1', 'CA_NS2', \n",
    "        'CA_NS3', 'CA_NS5', 'CA_Oas', 'CA_Obs', 'CA_Qfo', 'CA_SF2', 'CA_TP2', \n",
    "        'CA_TP3', 'CA_TPD', 'CH_Dav', 'CH_Lae', 'CN_Cha', 'CN_Qia', 'CZ_BK1', \n",
    "        'DE_Hai', 'DE_Lnf', 'DE_Tha', 'DK_Sor', 'FI_Hyy', 'FI_Let', 'FI_Var', \n",
    "        'FR_Fon', 'FR_LBr', 'IT_CA1', 'IT_CA3', 'IT_Col', 'IT_Lav', 'IT_Ren', \n",
    "        'IT_Ro1', 'IT_Ro2', 'IT_SRo', 'JP_SMF', 'RU_Fyo', 'SE_Kno', 'SE_Nor', \n",
    "        'UK_Ham', 'US_Blo', 'US_CMW', 'US_GLE', 'US_Ha1', 'US_Me3', 'US_MMS', \n",
    "        'US_NR1', 'US_Oho', 'US_Prr', 'US_Rpf', 'US_Syv', 'US_Uaf', 'US_UMB', \n",
    "        'US_UMd', 'US_WCr', 'AT_Neu', 'AU_DaP', 'AU_Fog', 'AU_Rig', 'AU_Stp', \n",
    "        'AU_Whr', 'AU_Wom', 'BR_Sa1', 'CA_NS6', 'CA_NS7', 'CH_Aws', 'CH_Fru', \n",
    "        'CH_Oe1', 'CH_Oe2', 'CL_SDF', 'CN_Dan', 'CN_Din', 'CN_Ha2', 'CN_HaM', \n",
    "        'CN_Sw2', 'CZ_BK2', 'CZ_wet', 'DE_Geb', 'DE_Gri', 'DE_Kli', 'DE_RuR', \n",
    "        'DE_SfN', 'DE_Spw', 'DE_Zrk', 'DK_Eng', 'ES_Amo', 'ES_LgS', 'FI_Jok', \n",
    "        'FI_Kaa', 'FI_Lom', 'FR_LGt', 'FR_Pue', 'GF_Guy', 'GL_NuF', 'GL_ZaH', \n",
    "        'IT_CA2', 'IT_Cp2', 'IT_Cpz', 'IT_MBo', 'IT_Noe', 'NL_Ca1', 'PA_SPs', \n",
    "        'PT_Esp', 'RU_Ha1', 'RU_Sam', 'RU_Tks', 'SE_Deg', 'SJ_Adv', 'US_ARb', \n",
    "        'US_ARc', 'US_ARM', 'US_Atq', 'US_EML', 'US_Goo', 'US_IB2', 'US_Ivo', \n",
    "        'US_KFS', 'US_KLS', 'US_Kon', 'US_Los', 'US_Myb', 'US_Ne2', 'US_Ne3', \n",
    "        'US_SRG', 'US_Sta', 'US_Tw1', 'US_Twt', 'US_Var', 'US_Whs', 'US_Wi6', \n",
    "        'US_Wi7', 'US_WPT']\n",
    "    \n",
    "        self.df = self.df[self.df['station'].isin(valid_stations)]\n",
    "\n",
    "    def drop_rows_with_value_counts(self, threshold=5):\n",
    "        \"\"\"\n",
    "        Drop rows in a Pandas DataFrame where values in a specific column\n",
    "        appear more than the specified threshold times.\n",
    "        \"\"\"\n",
    "        for station in self.df['station'].unique():\n",
    "            station_data = self.df[self.df['station'] == station]\n",
    "            value_counts = station_data['NEE'].value_counts()\n",
    "            values_to_remove = value_counts[value_counts > threshold].index.tolist()\n",
    "            self.df = self.df[~self.df['NEE'].isin(values_to_remove)]\n",
    "        self.df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    def plot_all_stations(self, df):\n",
    "        for category in df['station'].unique():\n",
    "            station_data = df[df['station'] == category]\n",
    "            biom = station_data['IGBP'].unique()\n",
    "            print(f'{category}')\n",
    "            print(biom)\n",
    "            plt.plot(station_data['NEE'])\n",
    "            plt.xlabel('Date')\n",
    "            plt.ylabel('Value')\n",
    "            plt.title(f'Station: {category}')\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "\n",
    "    def correlation(self, dataset, threshold):\n",
    "        col_corr = set()  # Set of all the names of correlated columns\n",
    "        corr_matrix = dataset.corr()\n",
    "        for i in range(len(corr_matrix.columns)):\n",
    "            for j in range(i):\n",
    "                if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value\n",
    "                    colname = corr_matrix.columns[i]  # getting the name of column\n",
    "                    col_corr.add(colname)\n",
    "        return col_corr\n",
    "\n",
    "    def plot_correlation_heatmap(self):\n",
    "        plt.figure(figsize=(20, 10))\n",
    "        sns.heatmap(self.df.corr(), annot=True, cmap='coolwarm', linewidths=0.5)\n",
    "        plt.title('Heatmap of Columns')\n",
    "        plt.show()\n",
    "\n",
    "    def replace_outliers_with_nan_rolling_iqr(self, window_size=3, scale=1.5):\n",
    "        new_df = self.df.copy()\n",
    "        for station in tqdm(self.df['station'].unique()):\n",
    "            station_data = self.df[self.df['station'] == station]\n",
    "            for column_name in station_data.columns:\n",
    "                if column_name not in ['Year','Month','lon','lat','station','IGBP'] and np.issubdtype(self.df[column_name].dtype, np.number):\n",
    "                    rolling_q1 = self.df[column_name].rolling(window=window_size, center=True).quantile(0.35)\n",
    "                    rolling_q3 = self.df[column_name].rolling(window=window_size, center=True).quantile(0.65)\n",
    "                    rolling_iqr = rolling_q3 - rolling_q1\n",
    "                    \n",
    "                    lower_bound = rolling_q1 - (scale * rolling_iqr)\n",
    "                    upper_bound = rolling_q3 + (scale * rolling_iqr)\n",
    "                    \n",
    "                    outliers = (self.df[column_name] < lower_bound) | (self.df[column_name] > upper_bound)\n",
    "                    \n",
    "                    new_df.loc[outliers, column_name] = np.nan\n",
    "            self.df = new_df\n",
    "\n",
    "    def mean_daily_data_by_month(self):\n",
    "        self.df['Date'] = pd.to_datetime(self.df['Date'])\n",
    "        df_date = self.df['Date']\n",
    "        category = self.df['station']\n",
    "        self.df.set_index('Date', inplace=True)\n",
    "\n",
    "        cleaned_df = pd.DataFrame()\n",
    "        for station in self.df['station'].unique():\n",
    "            # Select data for the current station\n",
    "            station_data = self.df[self.df['station'] == station]\n",
    "            numerical_columns = station_data.select_dtypes(include=['number']).columns\n",
    "            categorical_columns = station_data.select_dtypes(exclude=['number']).columns\n",
    "\n",
    "            # Extract year and month from the index\n",
    "            station_data['Year'] = station_data.index.year\n",
    "            station_data['Month'] = station_data.index.month\n",
    "\n",
    "            # Create the aggregation dictionary\n",
    "            agg_dict = {col: 'mean' for col in numerical_columns}\n",
    "            agg_dict.update({col: 'first' for col in categorical_columns})\n",
    "\n",
    "            # Group by year and month, and calculate the mean for numerical columns\n",
    "            result_df = station_data.groupby(['Year', 'Month']).agg(agg_dict).reset_index()\n",
    "            cleaned_df = pd.concat([cleaned_df, result_df])\n",
    "        return cleaned_df, df_date, category\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_preprocessing = DataPreprocessing(r'C:\\Univercity\\Skoltech\\NEE_2.0\\forest_meteo_gpp_nee_2024.csv', \n",
    "                                  r'C:\\Univercity\\Skoltech\\NEE_2.0\\NO_FOREST_meteo_gpp_nee_2024.csv')\n",
    "\n",
    "data_preprocessing.load_and_clean_data()\n",
    "data_preprocessing.drop_rows_with_value_counts()\n",
    "\n",
    "corr_features = data_preprocessing.correlation(data_preprocessing.df.drop(['NEE', 'GPP'], axis=1), 0.6)\n",
    "data_preprocessing.df.drop(corr_features, axis=1, inplace=True)\n",
    "\n",
    "cleaned_df, df_date, category = data_preprocessing.mean_daily_data_by_month()\n",
    "cleaned_df = cleaned_df.dropna(subset=['NEE'])\n",
    "cleaned_df.reset_index(drop=True, inplace=True)\n",
    "cleaned_df = cleaned_df.interpolate()\n",
    "\n",
    "valid_features = ['GPP', 'IGBP', 'Z0M', 'RHOA', 'PW', 'ALLSKY_SRF_ALB',\n",
    "                   'TS_RANGE', 'CLRSKY_SRF_ALB', 'WS50M_MIN', 'ZENITH_LUMINANCE']\n",
    "columns_to_select = valid_features + ['NEE']\n",
    "\n",
    "# Select the columns from the DataFrame\n",
    "cleaned_df= cleaned_df[columns_to_select]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the igbp_dict\n",
    "igbp_dict = {\n",
    "    0: \"Water Bodies\",\n",
    "    1: \"ENF\",  # Evergreen Needleleaf Forests\n",
    "    2: \"EBF\",  # Evergreen Broadleaf Forests\n",
    "    3: \"DNF\",  # Deciduous Needleleaf Forests\n",
    "    4: \"DBF\",  # Deciduous Broadleaf Forests\n",
    "    5: \"MF\",   # Mixed Forests\n",
    "    6: \"CSH\",  # Closed Shrublands\n",
    "    7: \"OSH\",  # Open Shrublands\n",
    "    8: \"WSA\",  # Woody Savannas\n",
    "    9: \"SAV\",  # Savannas\n",
    "    10: \"GRA\", # Grasslands\n",
    "    11: \"WET\", # Permanent Wetlands\n",
    "    12: \"CRO\", # Croplands\n",
    "    13: \"URB\", # Urban and Built-up Lands\n",
    "    14: \"CNV\", # Cropland/Natural Vegetation Mosaics\n",
    "    15: \"SNO\", # Permanent Snow and Ice\n",
    "    16: \"BAR\"  # Barren\n",
    "}\n",
    "\n",
    "# Invert the igbp_dict to map short names to integers\n",
    "igbp_dict_inverted = {v: k for k, v in igbp_dict.items()}\n",
    "\n",
    "# Replace the values in cleaned_df['IGBP']\n",
    "cleaned_df['IGBP'] = cleaned_df['IGBP'].replace(igbp_dict_inverted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features to drop\n",
    "features_to_drop = ['NEE']\n",
    "X = cleaned_df.drop(features_to_drop, axis=1)\n",
    "y = cleaned_df['NEE']\n",
    "\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Specify the categorical features\n",
    "categorical_features = ['IGBP']\n",
    "\n",
    "# Initialize MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Separate categorical and numerical features\n",
    "X_train_num = X_train.drop(columns=categorical_features)\n",
    "X_test_num = X_test.drop(columns=categorical_features)\n",
    "\n",
    "# Fit the scaler on the numerical training data and transform both training and testing numerical data\n",
    "X_train_num_scaled = scaler.fit_transform(X_train_num)\n",
    "X_test_num_scaled = scaler.transform(X_test_num)\n",
    "\n",
    "# Convert scaled numerical data back to DataFrame\n",
    "X_train_num_scaled_df = pd.DataFrame(X_train_num_scaled, columns=X_train_num.columns, index=X_train_num.index)\n",
    "X_test_num_scaled_df = pd.DataFrame(X_test_num_scaled, columns=X_test_num.columns, index=X_test_num.index)\n",
    "\n",
    "# Concatenate scaled numerical data with categorical data\n",
    "X_train_scaled = pd.concat([X_train_num_scaled_df, X_train[categorical_features]], axis=1)\n",
    "X_test_scaled = pd.concat([X_test_num_scaled_df, X_test[categorical_features]], axis=1)\n",
    "\n",
    "# Create Pool objects for training and testing\n",
    "train_pool = Pool(data=X_train_scaled, label=y_train, cat_features=categorical_features)\n",
    "test_pool = Pool(data=X_test_scaled, label=y_test, cat_features=categorical_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 1.9725284\ttotal: 170ms\tremaining: 2m 49s\n",
      "100:\tlearn: 1.1104046\ttotal: 2.85s\tremaining: 25.3s\n",
      "200:\tlearn: 0.9647791\ttotal: 5.45s\tremaining: 21.7s\n",
      "300:\tlearn: 0.8814545\ttotal: 8.22s\tremaining: 19.1s\n",
      "400:\tlearn: 0.8255284\ttotal: 10.7s\tremaining: 16s\n",
      "500:\tlearn: 0.7846615\ttotal: 13.4s\tremaining: 13.3s\n",
      "600:\tlearn: 0.7485362\ttotal: 16.2s\tremaining: 10.7s\n",
      "700:\tlearn: 0.7177649\ttotal: 19s\tremaining: 8.12s\n",
      "800:\tlearn: 0.6907762\ttotal: 21.9s\tremaining: 5.45s\n",
      "900:\tlearn: 0.6650012\ttotal: 24.8s\tremaining: 2.72s\n",
      "999:\tlearn: 0.6423793\ttotal: 27.3s\tremaining: 0us\n",
      "Mean Squared Error: 1.0592574590782255\n",
      "R-squared: 0.748411035716773\n"
     ]
    }
   ],
   "source": [
    "best_model = CatBoostRegressor(iterations=1000, learning_rate=0.1, depth=6, verbose=100)\n",
    "best_model.fit(train_pool)\n",
    "\n",
    "# Make predictions\n",
    "predictions = best_model.predict(test_pool)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "r2 = r2_score(y_test, predictions)\n",
    "\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "print(f'R-squared: {r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_model.save_model('Catboost_NEE_model.cbm')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
